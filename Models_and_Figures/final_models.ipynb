{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "from processing_helper_functions import final_processing\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables to choose which downsampling technique, time series encoding technique, and other settings to use\n",
    "\n",
    "# Which year of past data you would like to train on. One of: 0, 1, 2 where 0 is most recent and 2 is least recent\n",
    "# If you want to train on more than one year, enter the least recent year to include\n",
    "years_data = 0\n",
    "# Set to True if you want to only train on 1 year of data\n",
    "# Set to False to train on all data through years variable above\n",
    "only_one = True\n",
    "# Which time encoding technique should be used\n",
    "# One of: 'all', 'slopes', 'all_ma', 'sma', 'ema'\n",
    "encode_method = 'all'\n",
    "# Which set of features should be used\n",
    "# One of: 'baseline_demographics_cols', 'baseline_mci_cols', 'baseline_mmse30_cols', 'baseline_linear_selected_features',\n",
    "# 'simplified_cols_withapoe', 'simplified_cols_withoutapoe', 'all_cols'\n",
    "feature_set = 'all_cols'\n",
    "# Which downsampling technique should be used\n",
    "# One of: 'randdownsample', 'matcheddownsample', 'train', 'weighted'\n",
    "sample_type = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amokeson/efficient-dementia-prediction/Models_and_Figures/processing_helper_functions.py:177: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  constructed_data.set_value(i,str(j), patient.iloc[years-years_data][j])\n"
     ]
    }
   ],
   "source": [
    "encode_suffix = encode_method\n",
    "if encode_method in ['all','current']:\n",
    "    encode_suffix = str(years_data+1)\n",
    "if encode_method == 'ema':\n",
    "    encode_suffix = encode_method+str(half_life)\n",
    "if only_one:\n",
    "    save_suffix = '%s_%s_2yrprev_within3_singleyear%s'%(sample_type, feature_set, str(int(encode_suffix)-1))\n",
    "else:\n",
    "    save_suffix = '%s_%s_2yrprev_within3_yrsincluded%s'%(sample_type, feature_set, encode_suffix)\n",
    "\n",
    "file_suffix = \"2yrprev_within3\"\n",
    "with h5py.File(\"../DATA/PROCESSED/standardized_stacked_imputed/%s.h5\"%file_suffix, 'r') as hf:\n",
    "    ALL_SAMPLES = hf[\"samples\"][:]\n",
    "DATA = pd.read_csv(\"../DATA/PROCESSED/standardized/merged_kept_data_%s.csv\"%file_suffix, index_col=0)\n",
    "ALL_SAMPLES_df = pd.DataFrame(ALL_SAMPLES, columns=[\"projid\",\"fu_year\",\"onset_label_time\",\"onset_label_time_binary\"])\n",
    "    \n",
    "constructed_data = final_processing(encode_method, years_data, only_one, feature_set, DATA.columns[6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9135044642857143], [0.927246017253645], [0.7119457894753933]]\n"
     ]
    }
   ],
   "source": [
    "# GBDT\n",
    "accuracy_save = []\n",
    "roc_auc_save = []\n",
    "pr_auc_save = []\n",
    "precision_save = []\n",
    "recall_save = []\n",
    "fpr_save = []\n",
    "tpr_save = []\n",
    "\n",
    "if sample_type in ['randdownsample','matcheddownsample']:\n",
    "    train_samples_df = pd.read_csv(\"../DATA/PROCESSED/split_projids/%s_train_%s.txt\"%(sample_type,file_suffix),index_col=0).drop_duplicates()\n",
    "    train_samples_df[\"keep\"]=1\n",
    "    data_train = pd.merge(DATA, train_samples_df, on=['projid','fu_year'], how='left')\n",
    "    train_idx = (data_train[\"keep\"]==1).values\n",
    "else:\n",
    "    train_idx =ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/train_%s.txt\"%(file_suffix))).values\n",
    "valid_idx =ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/test_%s.txt\"%(file_suffix))).values\n",
    "\n",
    "label_col = np.where(ALL_SAMPLES_df.columns.values=='onset_label_time_binary')[0][0]\n",
    "label_tr = ALL_SAMPLES[train_idx][:, label_col]\n",
    "label_val = ALL_SAMPLES[valid_idx][:, label_col] \n",
    "data_tr = constructed_data[train_idx]\n",
    "data_val = constructed_data[valid_idx]\n",
    "\n",
    "if sample_type == 'weighted':\n",
    "    weight_tr = label_tr.astype(float)\n",
    "    weight_tr[weight_tr==0] = np.sum(weight_tr)/(weight_tr.shape[0]-np.sum(weight_tr))\n",
    "    dmat_train = xgb.DMatrix(data_tr.fillna(-999.0), label=label_tr, missing=-999.0, weight=weight_tr)\n",
    "else:\n",
    "    dmat_train = xgb.DMatrix(data_tr.fillna(-999.0), label=label_tr, missing=-999.0)\n",
    "dmat_test = xgb.DMatrix(data_val.fillna(-999.0), label=label_val, missing=-999.0)\n",
    "num_round = 50\n",
    "param = {'silent':1, 'min_child_weight':0.25, 'eta':0.1, 'max_depth': 4, 'objective': 'binary:logistic', 'eval_metric': ['error','logloss']}\n",
    "model = xgb.train(param, dmat_train, num_boost_round=num_round, evals=[(dmat_train,'train'), (dmat_test,'eval')], early_stopping_rounds=5, verbose_eval=False)\n",
    "\n",
    "predr = model.predict(dmat_test, ntree_limit=model.best_iteration)\n",
    "accuracy_save.append(metrics.accuracy_score(label_val,np.round(predr)))\n",
    "fpr, tpr, _ = metrics.roc_curve(label_val,predr)\n",
    "fpr_save = fpr\n",
    "tpr_save = tpr\n",
    "roc_auc_save.append(metrics.auc(fpr,tpr))\n",
    "precision, recall, _ = metrics.precision_recall_curve(label_val,predr)\n",
    "precision_save.append(precision)\n",
    "recall_save.append(recall)\n",
    "pr_auc_save.append(metrics.auc(recall, precision))\n",
    "new_results = [accuracy_save,roc_auc_save,pr_auc_save,precision_save,recall_save,fpr_save,tpr_save]\n",
    "pickle.dump(new_results, open('../results/xgb_%s.p'%(save_suffix), 'wb'))\n",
    "pickle.dump(model, open('../results/xgb_%s.dat'%(save_suffix), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear\n",
    "accuracy_save = []\n",
    "roc_auc_save = []\n",
    "pr_auc_save = []\n",
    "precision_save = []\n",
    "recall_save = []\n",
    "fpr_save = []\n",
    "tpr_save = []\n",
    "\n",
    "if sample_type in ['randdownsample','matcheddownsample']:\n",
    "    train_samples_df = pd.read_csv(\"../DATA/PROCESSED/split_projids/%s_train_%s.txt\"%(sample_type,file_suffix),index_col=0).drop_duplicates()\n",
    "    train_samples_df[\"keep\"]=1\n",
    "    data_train = pd.merge(DATA, train_samples_df, on=['projid','fu_year'], how='left')\n",
    "    train_idx = (data_train[\"keep\"]==1).values\n",
    "else:\n",
    "    train_idx =ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/train_%s.txt\"%(file_suffix))).values\n",
    "valid_idx =ALL_SAMPLES_df[\"projid\"].isin(np.loadtxt(\"../DATA/PROCESSED/split_projids/test_%s.txt\"%(file_suffix))).values\n",
    "\n",
    "label_col = np.where(ALL_SAMPLES_df.columns.values=='onset_label_time_binary')[0][0]\n",
    "label_tr = ALL_SAMPLES[train_idx][:, label_col]\n",
    "label_val =ALL_SAMPLES[valid_idx][:, label_col] \n",
    "data_tr = constructed_data[train_idx]\n",
    "data_val = constructed_data[valid_idx]\n",
    "\n",
    "if sample_type == 'weighted':\n",
    "    weight_tr = label_tr.astype(float)\n",
    "    weight_tr_val = np.sum(weight_tr)/(weight_tr.shape[0]-np.sum(weight_tr))\n",
    "    clf = LogisticRegression(class_weight={0: weight_tr_val, 1: 1})\n",
    "else:\n",
    "    clf = LogisticRegression()\n",
    "clf.fit(data_tr, label_tr)\n",
    "\n",
    "accuracy_save.append(clf.score(data_val, label_val))\n",
    "roc_auc_save.append(metrics.roc_auc_score(label_val, clf.decision_function(data_val)))\n",
    "pr_auc_save.append(metrics.average_precision_score(label_val, clf.decision_function(data_val)))   \n",
    "fpr_save, tpr_save, _ = metrics.roc_curve(label_val, clf.decision_function(data_val))\n",
    "precision_save, recall_save, _ = metrics.precision_recall_curve(label_val, clf.decision_function(data_val))\n",
    "\n",
    "new_results = [accuracy_save,roc_auc_save,pr_auc_save,precision_save,recall_save,fpr_save,tpr_save]\n",
    "pickle.dump(new_results, open('../results/linear_%s.p'%(save_suffix), 'wb'))\n",
    "pickle.dump(clf, open('../results/linear_%s.dat'%(save_suffix), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
