{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = \"2yrprev_within3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: the categorical ones are NOT the one-hot encoded version for the model, but the raw versions from before standardization\n",
    "\n",
    "cognitive_features = ['cts_animals', 'cts_bname', 'cts_catflu','cts_db', 'cts_delay', 'cts_df', 'cts_doperf', 'cts_ebdr', 'cts_ebmt',\\\n",
    "            'cts_fruits', 'cts_idea', 'cts_lopair', 'cts_mmse30', 'cts_nccrtd','cts_pmat', 'cts_pmsub', 'cts_read_nart', \\\n",
    "            'cts_sdmt', 'cts_story', 'cts_stroop_cname', 'cts_stroop_wread', 'cts_wli', 'cts_wlii', 'cts_wliii']\n",
    "medical_features_sums = ['med_con_sum_cum', 'vasc_3dis_sum', 'vasc_risks_sum']\n",
    "continuous_demographics = ['age_at_visit', 'educ']\n",
    "\n",
    "composite_vars = {\n",
    "    \"cogn_ep\": [\"cts_wli\", \"cts_wlii\", \"cts_wliii\", \"cts_ebmt\", \"cts_ebdr\",  \"cts_story\",\"cts_delay\"],\n",
    "    \"cogn_po\": [\"cts_lopair\", \"cts_pmat\"],\n",
    "    \"cogn_ps\": [\"cts_sdmt\", \"cts_nccrtd\", \"cts_stroop_cname\", \"cts_stroop_wread\"],\n",
    "    \"cogn_se\":  [\"cts_bname\", \"cts_catflu\", \"cts_read_nart\"],\n",
    "    \"cogn_wo\": [\"cts_db\", \"cts_df\", \"cts_doperf\"],\n",
    "    \"cogn_global\":  [\"cts_wli\", \"cts_wlii\", \"cts_wliii\", \"cts_ebmt\", \"cts_ebdr\",  \"cts_story\",\"cts_delay\",\\\n",
    "                     \"cts_lopair\", \"cts_pmat\", \"cts_sdmt\", \"cts_nccrtd\", \"cts_stroop_cname\", \"cts_stroop_wread\",\n",
    "                     \"cts_bname\", \"cts_catflu\", \"cts_read_nart\", \"cts_db\", \"cts_df\", \"cts_doperf\"] }\n",
    "    \n",
    "# these can stay as is\n",
    "binary = ['hypertension_cum', 'cancer_cum','diabetes_sr_rx', 'dm_cum', 'headinjrloc_cum', 'lostcons',\\\n",
    "                         'thyroid_cum', 'chf_cum', 'claudication_cum', 'heart_cum', 'stroke_cum', \"msex\", \"spanish\"]\n",
    "\n",
    "\n",
    "# these need to be one hot encoded\n",
    "categorical = ['apoe_4count', 'race', 'dcfdx']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data = pd.read_csv(\"../DATA/PROCESSED/standardized/merged_data_all_%s.csv\"%dset, index_col=0)\n",
    "\n",
    "sample_info = [\"projid\",\"study\",\"fu_year\",\"scaled_to\", \"onset_label_time\", 'onset_label_time_binary']\n",
    "data = load_data[sample_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = np.setdiff1d(load_data.columns, sample_info)\n",
    "features = load_data[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demographics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_features = pd.read_csv(\"../DATA/PROCESSED/merged_kept_data_2yrprev_within3.csv\").drop(['Unnamed: 0'], axis=1)\n",
    "orig_data_features[\"apoe_4count\"] = orig_data_features[\"apoe_genotype\"].apply(lambda x: 0 if x in [22., 23., 33.] else 1 if x in [24., 34.] else 2 if x == 44. else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features =  binary + categorical \n",
    "continuous_features = cognitive_features + continuous_demographics + medical_features_sums + list(composite_vars.keys())\n",
    "\n",
    "demo_feats = ['age_at_visit', 'msex','educ', 'race', 'spanish', 'apoe_4count']\n",
    "# cog_feats = ['dcfdx', 'cogn_ep', 'cogn_po', 'cogn_ps', 'cogn_se', 'cogn_wo','cogn_global']\n",
    "cog_feats = ['dcfdx', 'cogn_global']\n",
    "\n",
    "med_feats = ['med_con_sum_cum', 'vasc_3dis_sum', 'vasc_risks_sum', 'cancer_cum','claudication_cum', \\\n",
    "        'diabetes_sr_rx', 'dm_cum', 'headinjrloc_cum', 'heart_cum', 'hypertension_cum','stroke_cum', 'thyroid_cum'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_test_vars = ['age_at_visit', 'educ', 'cogn_ep', 'cogn_po', 'cogn_ps', 'cogn_se', 'cogn_wo','cogn_global']\n",
    "\n",
    "t_test_vars += ['med_con_sum_cum','vasc_3dis_sum', 'vasc_risks_sum']\n",
    "#u_test_vars = ['med_con_sum_cum','vasc_3dis_sum', 'vasc_risks_sum']\n",
    "    \n",
    "chi_test_vars = ['msex', 'race', 'spanish', 'apoe_4count', 'dcfdx', 'cancer_cum','claudication_cum', \\\n",
    "        'diabetes_sr_rx', 'dm_cum', 'headinjrloc_cum', 'heart_cum', 'hypertension_cum','stroke_cum', 'thyroid_cum'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "percentages=True\n",
    "\n",
    "for feat_group in [demo_feats, cog_feats, med_feats]:\n",
    "\n",
    "    for feat in feat_group:\n",
    "        controls = orig_data_features[data[\"onset_label_time_binary\"]==0][feat]\n",
    "        dementias = orig_data_features[data[\"onset_label_time_binary\"]==1][feat]\n",
    "\n",
    "        if feat in t_test_vars:\n",
    "\n",
    "\n",
    "            t,p = stats.ttest_ind(controls.dropna().values,dementias.dropna().values)\n",
    "            p_stars = \"***\" if p<.001 else \"**\" if p <.01 else \"*\" if p < .05 else \"\"\n",
    "\n",
    "            print(\"%s & $t=%.2f^{%s}$  & $%.2f \\pm %.2f$ &  $%.2f \\pm %.2f$  \\\\\\\\ \"%(feat.replace(\"_\", \"\\_\"),t,p_stars, controls.mean(), controls.std(), dementias.mean(), dementias.std()))\n",
    "\n",
    "        else:\n",
    "\n",
    "            # create dictionary of counts for observed values of feature\n",
    "            control_valcounts = {}\n",
    "            for i,v in enumerate(controls.value_counts().index):\n",
    "                control_valcounts[v] = controls.value_counts().values[i]\n",
    "            dem_valcounts = {}\n",
    "            for i,v in enumerate(dementias.value_counts().index):\n",
    "                dem_valcounts[v] = dementias.value_counts().values[i]\n",
    "\n",
    "            #get union of all values seen (just in case one of the groups has some 0s for some values)\n",
    "            all_vals = np.union1d(list(dem_valcounts.keys()), list(control_valcounts.keys()))\n",
    "            \n",
    "            # generate contingency table (shape: values observed x groups)\n",
    "            contingency_table = np.array([[control_valcounts[elt], dem_valcounts[elt]] for elt in all_vals])\n",
    "\n",
    "\n",
    "            chi2_stat, p, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "            p_stars = \"***\" if p<.001 else \"**\" if p <.01 else \"*\" if p < .05 else \"\"\n",
    "\n",
    "            \n",
    "            \n",
    "            if percentages:\n",
    "                if len(all_vals) < 3:\n",
    "                    controlfrac =control_valcounts[1]/np.sum(list(control_valcounts.values())) * 100\n",
    "                    demfrac =dem_valcounts[1]/np.sum(list(dem_valcounts.values())) * 100\n",
    "                    \n",
    "                    print(\"%s & $\\chi^2=%.2f^{%s}$  & $%.1f\\%%$ &  $%.1f\\%%$  \\\\\\\\ \"%(feat.replace(\"_\", \"\\_\"), chi2_stat,p_stars,controlfrac, demfrac))\n",
    "\n",
    "                \n",
    "                else:\n",
    "                    outcomes_str = \"/\".join(all_vals.astype(int).astype(str))\n",
    "                    \n",
    "                    controlvals = (np.round(contingency_table[:,0]/sum(contingency_table[:,0])*100,1))\n",
    "                    demvals = (np.round(contingency_table[:,1]/sum(contingency_table[:,1])*100,1))\n",
    "                    controlvals_str = \"/\".join([\"%s\\%%\"%x for x in controlvals.astype(str)])\n",
    "                    demvals_str = \"/\".join([\"%s\\%%\"%x for x in demvals.astype(str)])\n",
    "                \n",
    "                    print(\"%s ($%s$) & $\\chi^2=%.2f^{%s}$  & $%s$ &  $%s$  \\\\\\\\ \"%(feat.replace(\"_\", \"\\_\"),outcomes_str, chi2_stat,p_stars,controlvals_str, demvals_str))\n",
    "            else:\n",
    "                outcomes_str = \"/\".join(all_vals.astype(int).astype(str))\n",
    "                controlvals_str = \"/\".join(contingency_table[:,0].astype(int).astype(str))\n",
    "                demvals_str = \"/\".join(contingency_table[:,1].astype(int).astype(str))\n",
    "                \n",
    "                print(\"%s ($%s$) & $\\chi^2=%.2f^{%s}$  & $%s$ &  $%s$  \\\\\\\\ \"%(feat.replace(\"_\", \"\\_\"),outcomes_str, chi2_stat,p_stars,controlvals_str, demvals_str))\n",
    "    print(\"\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
